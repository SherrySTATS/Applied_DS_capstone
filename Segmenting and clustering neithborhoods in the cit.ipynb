{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Segmenting and Clustering Neighborhoods in Toronto"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In this assignment, we explore, segment, and cluster the neighborhoods in the city of Toronto. However, unlike New York, the neighborhood data is not readily available on the internet. What is interesting about the field of data science is that each project can be challenging in its unique way, so we need to learn to be agile and refine the skill to learn new libraries and tools quickly depending on the project.\n\nFor the Toronto neighborhood data, a Wikipedia page exists that has all the information we need to explore and cluster the neighborhoods in Toronto. You will be required to scrape the Wikipedia page and wrangle the data, clean it, and then read it into a pandas dataframe so that it is in a structured format like the New York dataset.\n\nOnce the data is in a structured format, you can replicate the analysis that we did to the New York City dataset to explore and cluster the neighborhoods in the city of Toronto."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Scrape the wikipedia page and wrangle the data, clean it, and then read it into a pandas dataframe"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "# import the library we use to open URLs\nimport urllib.request"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": "# specify which URL/web page we are going to be scraping\nurl = \"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_M\""
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "# open the url using urllib.request and put the HTML into the page variable\npage = urllib.request.urlopen(url)"
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Solving environment: done\n\n# All requested packages already installed.\n\n"
                }
            ],
            "source": "!conda install -c anaconda bs4 -y"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# import the BeautifulSoup library so we can parse HTML and XML documents\nfrom bs4 import BeautifulSoup"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# parse the HTML from our URL into the BeautifulSoup parse tree format\nsoup = BeautifulSoup(page, \"lxml\")"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "# use the 'find_all' function to bring back all instances of the 'table' tag in the HTML and store in 'all_tables' variable\nall_tables=soup.find_all(\"table\")\n"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "right_table=soup.find('table', class_='wikitable sortable')\n"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "A=[]\nB=[]\nC=[]\n\nfor row in right_table.findAll('tr'):\n    cells=row.findAll('td')\n    if len(cells)==3:\n        A.append(cells[0].find(text=True))\n        B.append(cells[1].find(text=True))\n        C.append(cells[2].find(text=True))"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": "import pandas as pd\ndf=pd.DataFrame(A,columns=['Postal Code'])\ndf['Borough']=B\ndf['Neighborhood']=C"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Only process the cells that have an assigned borough. Ignore cells with a borough that is Not assigned."
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": " df = df[~df.Borough.str.contains(\"Not assigned\")]"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(103, 3)"
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "df.shape"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}